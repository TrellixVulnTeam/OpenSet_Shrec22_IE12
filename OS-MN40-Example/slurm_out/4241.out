/home/pbdang/miniconda3/envs/openset/lib/python3.9/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
random seed: 641641.2861359119
Loader Initializing...

train samples: 2254
val samples: 568
Create new model
Feat_len =  2048
Epoch 0, Training...
	[0/281], Loss 2.2293
	[1/281], Loss 2.2771
	[2/281], Loss 2.0704
	[3/281], Loss 2.1116
	[4/281], Loss 2.0787
	[5/281], Loss 1.7256
	[6/281], Loss 1.6812
	[7/281], Loss 2.3979
	[8/281], Loss 1.6362
	[9/281], Loss 1.0544
	[10/281], Loss 1.1317
	[11/281], Loss 2.3118
	[12/281], Loss 2.7076
	[13/281], Loss 1.7925
	[14/281], Loss 2.5960
	[15/281], Loss 2.7092
	[16/281], Loss 1.6289
	[17/281], Loss 1.4661
	[18/281], Loss 1.8135
	[19/281], Loss 1.2855
	[20/281], Loss 1.5329
	[21/281], Loss 1.2846
	[22/281], Loss 1.1554
	[23/281], Loss 1.0425
	[24/281], Loss 0.9293
	[25/281], Loss 1.0342
	[26/281], Loss 0.7352
	[27/281], Loss 2.0694
	[28/281], Loss 0.8223
	[29/281], Loss 0.9597
	[30/281], Loss 0.6686
	[31/281], Loss 0.7690
	[32/281], Loss 0.8469
	[33/281], Loss 1.7392
	[34/281], Loss 0.6879
	[35/281], Loss 0.7090
	[36/281], Loss 0.8260
	[37/281], Loss 0.8570
	[38/281], Loss 1.2777
	[39/281], Loss 0.8383
	[40/281], Loss 0.4896
	[41/281], Loss 0.7728
	[42/281], Loss 0.6844
	[43/281], Loss 0.6015
	[44/281], Loss 0.4369
	[45/281], Loss 0.9536
	[46/281], Loss 1.0795
	[47/281], Loss 0.4840
	[48/281], Loss 0.6052
	[49/281], Loss 0.2648
	[50/281], Loss 0.4067
	[51/281], Loss 0.6630
	[52/281], Loss 0.4340
	[53/281], Loss 0.7637
	[54/281], Loss 0.2393
	[55/281], Loss 0.9357
	[56/281], Loss 0.4419
	[57/281], Loss 1.6107
	[58/281], Loss 1.4356
	[59/281], Loss 0.2283
	[60/281], Loss 0.5440
	[61/281], Loss 0.6128
	[62/281], Loss 0.5545
	[63/281], Loss 0.6368
	[64/281], Loss 0.5878
	[65/281], Loss 0.5776
	[66/281], Loss 0.4631
	[67/281], Loss 0.4114
	[68/281], Loss 0.5847
	[69/281], Loss 0.4528
	[70/281], Loss 0.4217
	[71/281], Loss 0.1145
	[72/281], Loss 0.6580
	[73/281], Loss 0.2630
	[74/281], Loss 0.2137
	[75/281], Loss 0.8724
	[76/281], Loss 0.9052
	[77/281], Loss 0.6187
	[78/281], Loss 0.3739
	[79/281], Loss 0.8524
	[80/281], Loss 0.1428
	[81/281], Loss 0.3109
	[82/281], Loss 0.5110
	[83/281], Loss 0.2921
	[84/281], Loss 0.8669
	[85/281], Loss 0.6085
	[86/281], Loss 0.3204
	[87/281], Loss 0.3080
	[88/281], Loss 0.9444
	[89/281], Loss 0.1720
	[90/281], Loss 1.0752
	[91/281], Loss 0.6991
	[92/281], Loss 0.2996
	[93/281], Loss 0.2367
	[94/281], Loss 0.7367
	[95/281], Loss 0.1665
	[96/281], Loss 0.6176
	[97/281], Loss 0.4533
	[98/281], Loss 0.2559
	[99/281], Loss 0.6338
	[100/281], Loss 0.4456
	[101/281], Loss 0.6830
	[102/281], Loss 0.1365
	[103/281], Loss 0.3627
	[104/281], Loss 0.0877
	[105/281], Loss 0.2371
	[106/281], Loss 0.8978
	[107/281], Loss 0.6107
	[108/281], Loss 0.4956
	[109/281], Loss 0.1803
	[110/281], Loss 0.4981
	[111/281], Loss 0.3504
	[112/281], Loss 0.8489
	[113/281], Loss 0.5181
	[114/281], Loss 0.6113
	[115/281], Loss 0.1567
	[116/281], Loss 0.1539
	[117/281], Loss 0.3706
	[118/281], Loss 0.5749
	[119/281], Loss 0.3703
	[120/281], Loss 0.2245
	[121/281], Loss 0.2354
	[122/281], Loss 0.3161
	[123/281], Loss 0.4129
	[124/281], Loss 0.1128
	[125/281], Loss 0.3321
	[126/281], Loss 0.1887
	[127/281], Loss 0.0674
	[128/281], Loss 0.0952
	[129/281], Loss 0.4562
	[130/281], Loss 0.2041
	[131/281], Loss 0.5201
	[132/281], Loss 0.2848
	[133/281], Loss 0.1443
	[134/281], Loss 0.3459
	[135/281], Loss 0.2224
	[136/281], Loss 0.1193
	[137/281], Loss 0.2450
	[138/281], Loss 0.3840
	[139/281], Loss 0.5562
	[140/281], Loss 0.1318
	[141/281], Loss 0.1115
	[142/281], Loss 0.4981
	[143/281], Loss 0.1356
	[144/281], Loss 0.3103
	[145/281], Loss 0.1391
	[146/281], Loss 0.7633
	[147/281], Loss 0.1277
	[148/281], Loss 0.2369
	[149/281], Loss 0.3888
	[150/281], Loss 0.0775
	[151/281], Loss 0.0568
	[152/281], Loss 0.1418
	[153/281], Loss 0.1465
	[154/281], Loss 0.1751
	[155/281], Loss 0.6338
	[156/281], Loss 0.2100
	[157/281], Loss 0.0233
	[158/281], Loss 0.1727
	[159/281], Loss 0.4395
	[160/281], Loss 0.4809
	[161/281], Loss 0.0828
	[162/281], Loss 0.1310
	[163/281], Loss 0.0972
	[164/281], Loss 0.0406
	[165/281], Loss 0.0704
	[166/281], Loss 0.2869
	[167/281], Loss 0.1111
	[168/281], Loss 0.1165
	[169/281], Loss 0.4520
	[170/281], Loss 0.5491
	[171/281], Loss 0.4225
	[172/281], Loss 1.2034
	[173/281], Loss 0.5380
	[174/281], Loss 0.2056
	[175/281], Loss 0.2428
	[176/281], Loss 0.0908
	[177/281], Loss 0.0640
	[178/281], Loss 0.1274
	[179/281], Loss 0.0496
	[180/281], Loss 0.1369
	[181/281], Loss 0.0618
	[182/281], Loss 0.0220
	[183/281], Loss 0.4415
	[184/281], Loss 0.3546
	[185/281], Loss 0.4418
	[186/281], Loss 0.3422
	[187/281], Loss 0.2230
	[188/281], Loss 0.0297
	[189/281], Loss 0.1460
	[190/281], Loss 0.1968
	[191/281], Loss 0.0962
	[192/281], Loss 0.1144
	[193/281], Loss 0.1911
	[194/281], Loss 0.0949
	[195/281], Loss 0.0909
	[196/281], Loss 0.0868
	[197/281], Loss 0.2607
	[198/281], Loss 0.3393
	[199/281], Loss 0.2908
	[200/281], Loss 0.1867
	[201/281], Loss 0.1909
	[202/281], Loss 0.2280
	[203/281], Loss 0.2883
	[204/281], Loss 0.0200
	[205/281], Loss 0.1096
	[206/281], Loss 0.1695
	[207/281], Loss 0.0997
	[208/281], Loss 0.0974
	[209/281], Loss 0.3662
	[210/281], Loss 0.0896
	[211/281], Loss 0.1360
	[212/281], Loss 0.3605
	[213/281], Loss 0.0566
	[214/281], Loss 0.1152
	[215/281], Loss 0.0820
	[216/281], Loss 0.7728
	[217/281], Loss 0.1013
	[218/281], Loss 0.1716
	[219/281], Loss 0.0606
	[220/281], Loss 0.1232
	[221/281], Loss 0.0463
	[222/281], Loss 0.3520
	[223/281], Loss 0.0929
	[224/281], Loss 0.0831
	[225/281], Loss 0.2089
	[226/281], Loss 0.1032
	[227/281], Loss 0.0611
	[228/281], Loss 0.0812
	[229/281], Loss 0.0722
	[230/281], Loss 0.3982
	[231/281], Loss 0.1782
	[232/281], Loss 0.1492
	[233/281], Loss 0.0463
	[234/281], Loss 0.1477
	[235/281], Loss 0.0303
	[236/281], Loss 0.1407
	[237/281], Loss 0.0493
	[238/281], Loss 0.1699
	[239/281], Loss 0.3642
	[240/281], Loss 0.2768
	[241/281], Loss 0.0299
	[242/281], Loss 0.4749
	[243/281], Loss 0.1592
	[244/281], Loss 0.0705
	[245/281], Loss 0.0671
	[246/281], Loss 0.0214
	[247/281], Loss 0.0667
	[248/281], Loss 0.1243
	[249/281], Loss 0.1401
	[250/281], Loss 0.1037
	[251/281], Loss 0.0609
	[252/281], Loss 0.2795
	[253/281], Loss 0.1488
	[254/281], Loss 0.2137
	[255/281], Loss 0.3218
	[256/281], Loss 0.0700
	[257/281], Loss 0.3074
	[258/281], Loss 0.1439
	[259/281], Loss 0.1323
	[260/281], Loss 0.1516
	[261/281], Loss 0.2035
	[262/281], Loss 0.0716
	[263/281], Loss 0.5979
	[264/281], Loss 0.0182
	[265/281], Loss 0.0922
	[266/281], Loss 0.0796
	[267/281], Loss 0.2425
	[268/281], Loss 0.0171
	[269/281], Loss 0.1408
	[270/281], Loss 0.0909
	[271/281], Loss 0.1068
	[272/281], Loss 0.0592
	[273/281], Loss 0.0663
	[274/281], Loss 0.1887
	[275/281], Loss 0.0804
	[276/281], Loss 0.1015
	[277/281], Loss 0.2132
	[278/281], Loss 0.1139
	[279/281], Loss 0.1854
	[280/281], Loss 0.0628
Epoch: 0, Time: 434.8202s, Loss: 0.478261, Tpl_Loss: 0.479350
overall acc  | meanclass acc  | 
0.8661       | 0.8155         | 
This Epoch Done!

Epoch 1, Training...
slurmstepd: error: *** JOB 4241 ON selab2 CANCELLED AT 2022-02-23T18:52:41 ***
